.hsk.tagged_diagram: [<currentTiddler>tags[]field:icon[_my/images/icons/diagram]]
.hsk.tagged_exemplar: [<currentTiddler>tags[]field:icon[_my/images/icons/exemplar]]
.hsk.tagged_feature: [<currentTiddler>tags[]field:icon[_my/images/icons/feature]]
.hsk.tagged_topic: [<currentTiddler>tags[]field:icon[_my/images/icons/topic]]
created: 20200309234237451
modified: 20200325163826219
tags: [[Lecture 1]] [[MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning]] [[Linear Algebra]]
title: Multiplying a Matrix by a Matrix
tmap.id: a2cb4c56-da80-4d20-81b2-316b6bdc7d84
type: text/vnd.tiddlywiki

$$
A B =
\begin{bmatrix}
       2 & 1 & 3  \\
       3 & 1 & 4  \\
       5 & 7 & 12 \\
\end  {bmatrix}

\begin{bmatrix}
       \text{(row 1)} \\
       \text{(row 2)} \\
       \text{(row 3)} \\
\end  {bmatrix}
$$


How do you multiply?

* [[Dot Product]] (row * column) - ''low level way!''<br>
* Columns $$\times$$ rows: $$

\begin{bmatrix}
       2 & 1 & 3  \\
       3 & 1 & 4  \\
       5 & 7 & 12 \\
\end  {bmatrix}

\begin{bmatrix}
       \text{(row 1)} \\
       \text{(row 2)} \\
       \text{(row 3)} \\
\end  {bmatrix} =

\begin{bmatrix} 2\\3\\5  \end{bmatrix} [\text{(row 1)}]+
\begin{bmatrix} 1\\1\\7  \end{bmatrix} [\text{(row 2)}]+
\begin{bmatrix} 3\\4\\12 \end{bmatrix} [\text{(row 3)}]
$$

A product of two [[matrices|Matrix]] is just a sum of [[Outer Products|Outer Product]]!

$$\displaystyle AB = \sum_{k=1}^n \text{(col k)}_A\text{(row k)}_B$$

It should be noted that each of these [[Outer Product]] [[matrices|Matrix]] has [[Rank]] = 1.

Let's compare the amounts of multiplications required to multiply two matrices $$(m \times n)(n \times p)$$ using both of these ways:

* Using [[Dot Product]] it's //n(mp)//: //n// for each of the dot products, and //mp// is the number of dot products needed, since the resulting matrix should have the size $$(m \times p)$$
* Using the [[Outer Products|Outer Product]] it's //(mp)n//: //mp// for each outer product, and we have //n// outer products

So computationally they're completely equivalent!!